{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEV Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL 1:\n",
    "- Download all files from Google Drive\n",
    "- From those files, keep only .csv files\n",
    "- Download table from Bigquery with already uploaded files\n",
    "- Compare files that have been already uploaded to Bigquery by taking its name\n",
    "- Keep new files\n",
    "- Upload new files to CS\n",
    "- Upload tracker_registry table with files uploaded\n",
    "\n",
    "ETL 2: Same as batch-telecom-data\n",
    "- Once a file enters on Cloud Storage it triggers the CloudFunction:\n",
    "    - It takes the csv file and applies the pandas transformation!!!\n",
    "    - It updates the tracker/registry files table with the file_name and the rows of it\n",
    "    - It uploads the data of the file to BQ table \n",
    "\n",
    "ETL 3: Checking function between Storage and BQ\n",
    "\n",
    "BQ table structures:\n",
    "    - BQ main table: columns needed / path_file / timestamp\n",
    "    - BQ tracker table: file_path / num_lines / timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENTS:\n",
    "- It is necessary to give access to the service_account to the drive_folder where the data is stored (now in mgaleramunoz drive)\n",
    "- Change project name: e3escoles-dev => e3escoles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#    - change cs_client\n",
    "#    - change bq_client\n",
    "#    - change destination_file for building drive client\n",
    "#    - change local_file_path for saving files in /tmp/\n",
    "\n",
    "\n",
    "# Perhaps we need to erase files when downloaded to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import io\n",
    "import csv\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google.cloud import storage, bigquery\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from utils import keep_csv_files, get_drive_files, find_index_of_substring, blob_to_filename, download_file_from_drive, count_csv_length\n",
    "from utils_gcloud import load_json_from_cs, _storage_connection, download_storage_file, _bigquery_connection, insert_data_to_bq, list_blobs, create_folder, upload_storage_file, insert_row_to_bq\n",
    "\n",
    "# Local testing var:\n",
    "json_key_path = 'config/e3escoles-aed88f04ce6d.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define general env vars:\n",
    "BUCKET_NAME = os.environ.get('BUCKET_NAME', 'ordino')\n",
    "CONFIG_FILE_NAME = os.environ.get('CONFIG_FILE_NAME', 'config/config.json')\n",
    "\n",
    "# Build Storage Client:\n",
    "cs_client = _storage_connection(json_key_path) # cs_client = storage.Client()\n",
    "bq_client = _bigquery_connection(json_key_path) # bq_client = bigquery.Client()\n",
    "\n",
    "# Load config vars:\n",
    "config = load_json_from_cs(cs_client, BUCKET_NAME, CONFIG_FILE_NAME)\n",
    "\n",
    "project_id = config['project_id']\n",
    "service_account_file_name = config['service_account_file_name']\n",
    "bq_dataset = config['bq_dataset']\n",
    "bq_tracker_table = config['bq_tracker_table']\n",
    "schema_tracker_path = config['schema_file_names']['schema_tracker_file']\n",
    "data_blob = config['storage_blobs']['data_blob']\n",
    "tracker_table_headers = config['tracker_table_headers']\n",
    "\n",
    "# Create vars: \n",
    "destination_file = './config/key_file.json' # destination_file = '/tmp/key_file.json'\n",
    "tracker_table_path = project_id + '.' + bq_dataset + '.' + bq_tracker_table\n",
    "\n",
    "\n",
    "# Build Google Drive Client:\n",
    "file = open(destination_file, 'wt', encoding='utf-8')\n",
    "service_account_json = download_storage_file(cs_client, BUCKET_NAME, service_account_file_name, destination_file)\n",
    "creds = Credentials.from_service_account_file(destination_file)\n",
    "drive_client = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# Get Drive files:\n",
    "files = get_drive_files(drive_client)\n",
    "csv_files = keep_csv_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query `tracker_registry` table in order to obtain new files:\n",
    "query = \"select file_name, num_lines from  \" + tracker_table_path\n",
    "query_job = bq_client.query(query)\n",
    "rows = query_job.result()\n",
    "bq_tracker_results = {}\n",
    "bq_tracker_files = []\n",
    "for row in rows:\n",
    "    bq_tracker_results[row.file_name] = [row.num_lines]\n",
    "    bq_tracker_files.append(row.file_name)\n",
    "\n",
    "# Compare both lists and filter already uploaded files\n",
    "for file in bq_tracker_files:\n",
    "    for index, csv in enumerate(csv_files):\n",
    "        if file == csv['name']:\n",
    "            csv_files.pop(index)\n",
    "\n",
    "# Determine year we are at\n",
    "current_year = datetime.datetime.now().year\n",
    "previous_year = current_year - 1\n",
    "current_year_str = '_' + str(current_year) + '-'\n",
    "previous_year_str = '_' + str(previous_year) + '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploads all the files into the data/.../ folder in Cloud Storage\n",
    "for index, file in enumerate(csv_files):\n",
    "    # Clculate index of our substring:\n",
    "    index_string = find_index_of_substring(file['name'], current_year_str)\n",
    "    if index_string == -1:\n",
    "        index_string = find_index_of_substring(file['name'], previous_year_str)\n",
    "    \n",
    "    # Extract date of the file\n",
    "    date_file = file['name'][index_string+1:index_string+11]\n",
    "\n",
    "    # Download file from Drive:\n",
    "    local_file_path = './data_samples/' + file['name'] # local_file_path = '/tmp/' + file['name']\n",
    "    download_file_from_drive(drive_client, file['id'], local_file_path)\n",
    "    \n",
    "    # Count rows of csv file:\n",
    "    len_file = count_csv_length(local_file_path)\n",
    "    current_tmstmp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    #if date_file in tmstmp_folders:\n",
    "    storage_blob_path = data_blob + date_file + '/' + file['name']\n",
    "    response = upload_storage_file(cs_client, BUCKET_NAME, storage_blob_path, local_file_path)\n",
    "\n",
    "    if response == \"Upload done succesfully\":\n",
    "        tracker_data = [file['name'], len_file, current_tmstmp]\n",
    "        row_data = dict(zip(tracker_table_headers, tracker_data))\n",
    "        insert_row_to_bq(client=bq_client, table_id= tracker_table_path, row=row_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feat: Add values to `tracker_registry` BQ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Add example rows to bq tracker_registry table:\n",
    "current_time = datetime.datetime.now()\n",
    "tmstmp1 = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "tmstmp2 =  (current_time - datetime.timedelta(days = 1)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "tmstmp3 = (current_time + datetime.timedelta(days = 1)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "dict_data = {\n",
    "    'file_name': ['03AAB_1rE_2022-11-07T14_47_08+0100.csv', '03A52_biblioteca_2022-11-07T14_41_42+0100.csv', '03A49_seminari3_2022-11-07T14_53_21+0100.csv'],\n",
    "    'num_lines': [43, 1003, 99988],\n",
    "    'tmstmp': [tmstmp1, tmstmp2, tmstmp3]\n",
    "}\n",
    "dataframe_test = pd.DataFrame(dict_data)\n",
    "dataframe_test\n",
    "\n",
    "#### Insert the test data to BQ tracker_registry table:\n",
    "bq_client = _bigquery_connection(json_key_path) # bq_client = bigquery.Client()\n",
    "schema_tracker = load_json_from_cs(cs_client, BUCKET_NAME, schema_tracker_path)\n",
    "schema = [bigquery.SchemaField(name=s['name'], field_type=s['type'], mode=s['mode']) for s in schema_tracker]\n",
    "\n",
    "num_rows_added = insert_data_to_bq(bq_client, schema, dataframe_test, bq_dataset, bq_tracker_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
